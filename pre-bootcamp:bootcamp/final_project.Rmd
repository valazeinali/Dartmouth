---
title: "R Boot Camp Problem Set"
author: "Carly Bobak"
date: "August 10, 2020"
output: pdf_document
---

```{r, setup, include=FALSE}
## If you have never created an R Markdown document before, go to
## File -> New File -> R Markdown -> click "Yes"
knitr::opts_knit$set(root.dir = "/Users/valazeinali/Desktop/Dartmouth/pre-bootcamp:bootcamp") #set the correct folder here so that it can be accessed in all your code chunks
knitr::opts_chunk$set(echo = TRUE)

r <- getOption("repos")
r["CRAN"] <- "http://cran.cnr.berkeley.edu/"
options(repos = r)

```

Establishing reliable biomarkers for assessing and validating clinical diagnosis at early prodromal stages of Parkinson's disease is crucial for developing therapies to slow or halt disease progression. This data set uses whole blood gene expression profiling from over 500 individuals where we will attempt to find a gene signature. This repository contains the gene expression profiles collected in the GENEPARK consortium. The main study sought a classifier for IPD. These data contain 233 healthy controls, 205 IPD patients, and 48 patients with other neurodegenerative diseases (NDD). Other samples are available in the data and can be used for additional analyses. The largest class of these additional samples are 22 samples from genetic unaffected controls and 41 genetic PD patients.

Note: the original study which uploaded this data to NIH Geo is not yet published.

\section{Data Wrangling}

Let's start by loading in our data sets. Download these from the sharepoint site, and make a new folder for R bootcamp. We'll switch to this directory here. 

```{r setup packages, message=FALSE, warning=FALSE, results=FALSE, echo=FALSE}

#uncomment these for first runthrough

install.packages("tinytex")
tinytex::install_tinytex()  # install TinyTeX
install.packages("tidyverse") 
```

Note that we have both a phenotype file, as well as a file which includes the normalized and log transformed expression values. We can use the read.csv function to load in these files. 

```{r load data}
pheno<-read.csv("parkPheno.csv")
expr<-read.csv("simulatedData.csv")
```

We should start by summarizing both these files. Try the following functions: head(), and View(). Note that while the dimensions on our phenotype file are reasonable, we have 552 columns in our expression file. Just summarize the first 10 columns of this file. 

```{r check out the data}

## Enter your own code here
head(pheno)
View(pheno)
summary(expr[1:20668,1:10, drop=FALSE])
```

Try summarizing the phenotype data:

```{r summarize the data}

## Enter your own code here
summary(pheno)

```

We make the following observations. 

\begin{enumerate}
\item We have some unnecessary data in this file. We aren't interested in the submission and last update date. We can reduce the dimensions of this file so it handles nicer from now on. 
\item We have a LOT of missing data. You'll learn how to handle this in some of your biostats classes! For now, we'll run what analyses we can given the data we have.
\item Some of our scores have been read in as character values (and they should be numbers). If you investigate this further, you'll find that some values have been recorded as "ND", which we'll assume means "no data". We will need to record these as NA values in R. 
\end{enumerate}

Our next step is to address item one. We will reduce the dimensions of our pheno data frame to include only that information that we're interested in modelling. We can exclude the dates, type (as it's all RNA), tissue (all whole blood), organism (all homo sapiens), and subject ID (we will be using geo_accession as our unique indicator). As well, we will exclude mutated_pd_genes, as we indend to define our own gene signature later this week.

Subset your pheno data frame to include columns 1,8,9,11:20.

```{r reduce pheno} 
library(tidyverse)
## Enter your own code here FIX
new_pheno <- pheno %>% select(1, 8,9,11:20)
new_pheno
```

Next we need to correct the columns which contain "ND". You can use the "which" function to find the index of of the matrices which are "ND", and then set these to NA. Set columns 8,9,11,12,13 to numeric values using the "as.numeric" function inside a "sapply" loop. Run a summary of the data frame again.

```{r replace ND, warning=F}
index<-which(pheno==" ND",arr.ind = T)
pheno[index]<-NA
j<-c(8,9,11,12,13)
pheno[,j]<-sapply(unlist(pheno[,j]),as.numeric)
summary(pheno)
```

Let's look at a summary of the first 10 columns of expression data set.

```{r sum expr}

## Enter your own code here
summary(expr[1:20668,1:10, drop=FALSE])

```

We don't need the X1 variable - this is just remaining row labels in the csv file. Let's remove this variable. 

```{r expr clean}

## Enter your own code here
expr_new = subset(expr, select = -c(X) )
head(expr_new[1:6,1:10, drop=FALSE]) # just to see if "X" gets dropped 
```

We don't see any evidence of missing values in our summary, but we should check all of the columns (excluding the ProbeID and GeneName). You can check this with the "anyNA"" function. 

```{r check NA}

## Enter your own code here
expr_na_CheckPrep = subset(expr_new, select = -c(GeneName)) # temp dropping these for N/A check
anyNA(expr_na_CheckPrep, recursive = FALSE)

```

Let's identify how big this problem is, and where it occurs.

```{r find NA}
which(is.na(expr),arr.ind = T)
```

So one of our gene names is NA! This isn't useful, so let's remove this row.

```{r remove NA}

## Enter your own code here
library(dplyr)
expr <- expr %>% slice(-c(20668))

```

We should see if the unique identifiers in our two data sets match. Check for a perfect match using the "identical" function.

```{r identifier match}
#STOPPED HERE
identical(colnames(expr[,-1]),as.character(pheno[,1]))
```

Question: why is the '-1' necessary here? Answer below!





So that we don't lose any work, let's clean up our workspace to include only our cleaned expression and pheno data sets, which we can reload later. 

```{r save workspace, echo=F}
rm(j)
rm(index)
save.image(file="RbootcampDay1.RData")
```


\section{Exploratory Data Analysis}

In this section we are going to explore some of the data we have, and maybe develop a diagnostic signature for Parkinson's disease.

First, load in your data from yesterday. 

```{r load day 1, echo=F}
load("RbootcampDay1.RData")
```

Let's re-examine our pheno data set with the summary function again. 

```{r pheno sum}

## Enter your own code here
summary(pheno)


```

We need to further delve into our disease label in order to simplify some of this analysis. Attach your pheno data frame using the attach function, and then summarize the disease label vector.

```{r disease lab, warning=F}

## Enter your own code here
attach(pheno)
summary(disease_label)
```

Here we have the counts of all the diseases in our data set. If you look at the actual excel file (not the csv), I've put in a dictionary for these acronyms if you're curious. Here, our controls and our genetic unaffected are both considered to be healthy controls. Any label which contains PD is some subset of Parkison's Disease, and the other labels represent other neurological disorders. We need to make a variable which records a 1 for our cases, and a 0 for our controls. Here, since we are interested in a signature that distinguishes PD from our other disease, the other diseases are technically part of the control set.

Try to set your case control vector using the grep function to find the indicies which contain "PD". At the end, sum your case vector to check that it worked. Make another variable of the words "case" and "control"

```{r case vector}
## Enter your own code here
pheno$PD <-grepl("PD", pheno$disease_label) # Label True for PD and False for no PD and store it into a column named "PD" in our pheno dataframe
pheno$PD <- as.numeric(pheno$PD) # Convert TRUE TO 1 and FALSE to 0

# 1: Parkison's Disease
# 0: NOT Parkison's Disease
table(pheno$PD)

```

We need to find differentially expressed genes. You'll learn more about this later. For now, feel free to use some of my code. Start by downloading the limma package

```{r limma,warning=F, message=F, results=F}
## If using Windows, first go to https://cran.rstudio.com/bin/windows/Rtools/ and install the appropriate version of Rtools
if (!requireNamespace("BiocManager", quietly = TRUE))
    install.packages("BiocManager")

#uncomment to load limma for first run through

BiocManager::install("limma")
library(limma)
```

We will use the following code. Please add comments to every line to tell me what its doing!

```{r find some genes}
## Subset our data for a training and test set
set.seed(2) #two random numbers generated for simulation
prob<-runif(ncol(expr)-2) #generates random deviates of the uniform distribution
k<-which(prob>=0.3333333) # stores indexes that have a prob > 1/3
eset<-expr[,2:ncol(expr)] #  Explanation Below 
eset<-eset[,k] # Stores the rows that meet the threshold of  prob > 1/3
rownames(eset)<-expr[,1] # take the row names from expr dataframe and put them in eset dataframe
design <- model.matrix(~0+as.factor(pheno$PD[k])) # #creating a design matrix and getting the independent variable ready for modeling 
fit <- eBayes(lmFit(eset,design)) # fitting the model with the parkinsons labels
topTable(fit, coef=2) # testing
results<-topTable(fit, coef=2, number=Inf) # showing the inferential stats
```

Here, we have our gene names, our log fold change for expression, average expression, t statistic, pvalue, adjusted pvalue (for multiple testing!!), and the log odds of differential expression.

Next, we select those genes that have adjusted p-values below 0.001. Again, add comments to every line to describe what the code is doing. 

```{r filter some genes}
selected  <- row.names(results)[p.adjust(results$P.Value, method="fdr")<0.001] # gets the characters that have a p-value of less than .001
direction <- sign(results$logFC) # positve or negative momentum (1,-1)
esetSel <- eset[selected, ] #storing the occurences of <.001 into esetSel 
nrow(esetSel) # how many occurences of <.001
```

Okay! So we're now looking at just 173 probes!

We are going to make a heat map here. I've provided the code, but try changing colours, labels, etc. to make it your own. You can try typing '?heatmap' into the console to see the help page and provide more ideas for what you'd like to change!

```{r make heat map, fig.height=10}
patientcolors <-ifelse(pheno$PD[k]==1,"blue","orange")
heatmap(as.matrix(esetSel), col=topo.colors(200), ColSideColors=patientcolors, distfun = function(x) dist(x,method = 'euclidean'))
```

Notice the annotation bar along the top. This indicates PD vs not PD samples. This heat map is an example of a 'non-supervised method' - where we didn't feed the labelled data to the algorithm. Instead, it is just clustering similar samples together. Because all of our PD samples cluster away from the non-PD samples, we are relatively certian we've picked good biomarkers! We should also check a PCA plot.

```{r make PCA}
pc<-prcomp(t(esetSel),center=T,scale=T)
plot(pc,type="l",main="Checking the number of Principle Components")
```



Again, I've provided code for you here. Change it to something you like better!


```{r pca plpt, fig.height=10, warning=FALSE, message=FALSE}
#install.packages("devtools")
#library(devtools)

install.packages("ggpubr")
library(ggpubr)
 
#install_github("vqv/ggbiplot")
#library(ggbiplot)

source("ggbiplot.R")
#library(ggbiplot)
g <- ggbiplot(pc, obs.scale = 1, var.scale = 1, 
              groups = as.factor(pheno$PD[k]), ellipse = F, 
              circle = F, labels=pheno$disease_label[k],var.axes = F)
g <- g + scale_color_discrete(name = '')
g <- g + theme(legend.direction = 'horizontal', 
               legend.position = 'top')
g <- g + theme_classic2()
print(g)
```

We have separation! Notice the obvious differences between cases and controls.

Make a variable which only contains the differential gene names and call it diffGenes AND print out all of these gene names using one line of code. The parentheses around the full line of code do this!

```{r get gene names}
diffGenes<-selected
diffGenes
```

To use these genes as a classifier, we will need to define a score function. Our score will be the sum of the average expression for the upregulated (positive) genes and the average for the down regulated (negative) genes. Here, I've written you a function which will do this. Please enter it and make comments to show you understand what its doing. 

```{r make function}
PDscore<-function(x,g,v,s){
  #x expression values for a sample
  #g all the genes
  #v the diffGenes
  #s is the sign of the logFC
  
  i<-which(g%in%v)
  x<-x[i] # stores the value at the ith index into x (for expression values for a sample)
  s<-s[i] # stores the value at the ith index into s (for sign of LogFC)
  p<-c() # column names
  n<-c() # column names
  for(i in 1:length(x)){ # loop through the entire expression values 
    if(s[i]>0){ # if the LogFC sign is positive than append it to the list p for positive
      p<-append(p,(x[i]))
    }
    else if(s[i]<0){ 
      n<-append(n,(x[i])) # if the LogFC sign is negative than append it to the list n for negative
    }
  }
  
  if(is.null(p)){p[1]=0} # null checks
  if(is.null(n)){n[1]=0}
  
  score<-mean(p)-mean(n) # the score is the differential of the mean of positive and negative
  return(score)
}
```

Now we can apply our function to our expression set to define a score for each patient. Comment what this is doing and why each step is necessary!

```{r apply function}
score<-c()

allGenes<-as.character(expr[as.character(expr$GeneName)%in%rownames(results),1])

for(i in 1:ncol(eset)){
  score[i]<-PDscore(eset[,i],allGenes,diffGenes,direction)
}

hist(score,main="Distribution of our PD Scores")
```

Now we'll use ggplot to make and interpret a violin plot of our score. I've provided some code to do this, but try to change labels, colours, etc. to make it your own. 

```{r violin plot, fig.height=10, fig.width=7, warning=F, message=F}
df<-data.frame(cbind(pheno$PD[k],score))

dp <- ggplot(df, aes(x=as.factor(pheno$PD[k]), y=score, fill=as.factor(pheno$PD[k]))) +
  geom_violin(trim=FALSE)+
  geom_boxplot(width=0.1, fill="white")+
  labs(title="Plot of case by score",x="Case ", y = "Score")+
  stat_compare_means(label.x = 1.5, label.y = 1, size=10)+
  stat_compare_means(aes(label = ..p.signif..), label.x = 1.5, label.y = 0.9, size =10) 
dp + theme(text = element_text(size = 18),)
```

This shows not only the boxplot of our data, but also the distribution of our data points around the boxplot! As before, we can see that we don't have significant separation for our score, although we can see that the cases are trending to have a higher score. With more time and data cleaning we may be able to find something here!

Let's make an ROC plot, first with our training data, and then with our test data. As before, play with the plot options to make something you like! Note, there are MANY packages to build ROC plots, this one is just simple. Feel free to play with other packages to make publication ready plots if you'd like!

```{r ROC plot 1, warning=F, message=F}
#install.packages("verification")
#install.packages("pROC")
library("pROC")
testEset<-expr[,2:ncol(expr)]
testEset<-testEset[,-k]
newScore<-apply(testEset,2,FUN=PDscore,allGenes,diffGenes,direction)
plot.roc(case[k]~score, data=df,legacy.axes=F,print.auc=T, ci=T, main="AUC for Diagnostic Score")
plot.roc(case[-k]~newScore,data=data.frame(cbind(case[-k],newScore)),add=T,print.auc=T, ci=T, col="blue", print.auc.y=0.4)
legend("topleft",c("Training Data","Test Data"),lty=c(1,1),col=c("black","blue"))
```

Notice that our score does better with our training data - this is expected! This is why we need to split our data, to avoid problems with over-fitting. These scores are better than random (the grey line), but we'd like to see an AUC as close to 1 as possible. Let's See if we can do better!

\section{Statistics!}

We can run a t-test to see if our score is significantly different between cases and controls. Try using the t.test function in R.

```{r t test}
allScore<-c(score,newScore)
mergeCase<-c(case[k],case[-k]) ## to preserve order

## Do the t.test here

```

The mean scores for our cases and controls are close, but they are significantly different with an extremely small p-value of 2.787e-13. This highlights a classical statistical fallacy - while small p-values are great, they are often meaningless without a large enough effect size. Here, we have achieved significance due to the large sample size of our study, hence our study is adequately powered. 

We could also run a simple regression to examine the impact of the score on the log odds of being a case. 

```{r small model}
smallModel<-glm(case[k]~score, family=binomial)
summary(smallModel)
```

Summarize this output!

Again, we conclude that the score is a statistically significant indicator of the odds of having PD. Let's build a larger model which examines other phenotype variables.

First, build a data frame which includes all the model data we're interested in. Start with the age variables in your pheno set, and then use the cbind() function to add on our scores and the binary case vector. Print a summary of the model data.

```{r subset exp}

## Enter your own code here

```

We should examine the correlations in our data set. You can do this quickly by building a correlation plot matrix. 

```{r corr plot, fig.height=10, warning=F, message=F}
#install.packages("corrplot")
library(corrplot)
M<-cor(na.omit(modelData))
corrplot.mixed(M)
```

How would you interpret this output? Write a few sentences below!

Let's build our first model. Here, we consider the case as our dependent variable, and the others as our explanatory variables. 

```{r first model}
model1<-glm(case[k]~.,family=binomial,data=modelData)
```

This error is important - it represetns that our model is drastically overfit. We can easily fix this using the BayesGLM model from the arm package

```{r bayes model, warning=FALSE, message=FALSE}
#install.packages("arm")
library(arm)
model1<-bayesglm(case[k]~.,family=binomial,data=modelData)
summary(model1)
```

We cannot use the step function for bayes glm. We will iteratively remove variables with the highest p-values, and then rerun the model. 

Try this on your own, removing one by one and checking the output to find the next largest p-value. OR if you feel up to the challenge, write your own function to automate this process for you! There are bonus points available ;)

```{r reduce bayes}

## Enter your own code here

```

This is our final model! Notice that our largest effect size is controlled by our genetic score. At a first glance, we might assume this means that the score has the largest effect on the model. However, if we recall how to interpret our coefficients, the estimated effect size is the change in log odds of being a case for a 1 unit increase in our score. Think about the score distribution: the range of our scores is fairly small. In contrast, the range of the updrs scores varies from 0 to 36. Keep in mind the scale of our data when interpretting these models!

Let's predict the probability of having a case given our model. Make a histogram of the score from this model. 

```{r predict with model,warning=F}

## Enter your own code here

```

Like before, we'll build a violin plot to compare the output of our regression model. See if you can adapt the violin plot code from before to do this now. 

```{r violin plot2, fig.height=10}

## Enter your own code here

```

Now we're starting to see a clearer separation of scores! It's clear that by including the established tests to pre-screen patients for PD and other neurological diseases we have improved overall performance. While this may be an obvious conclusion, it is worth noting that the context with which our diagnostic signature would be used would be on patients already exhibiting potential PD symptoms. Clearly this needs a little more work, but for a first pass at assessing raw data, it's not bad!


Again, we can examine ROC curves. I've done some of the set up to get the data in the right format. Use the ROC code above to then build your own plot!

```{r ROC plot 2, warning=F}
library("pROC")
nd<-cbind(pheno[-k,4:ncol(pheno)],newScore)
colnames(nd)<-c(colnames(modelData[1:ncol(modelData)-1]),"score")
newMScore<-predict(model2,newdata=nd)

## Enter your own code here

```

Here, we have a notable increase in AUC, particularly for our training data. Our test data shows an overal improvement as well, although with a large confidence interval. There are clearly some data points in here which are abnormal - and perhaps worth investigating.

##Congratulations, you have finished the R Bootcamp Assignment!